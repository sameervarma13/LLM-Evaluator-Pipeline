# LLM Evaluation Framework

This project provides a framework for evaluating the performance and consistency of large language models (LLMs) across different prompts, tasks, or generations. It is built using Jupyter notebooks for interactive analysis and is designed to support structured comparison, metric tracking, and qualitative review.

## Features

- Run multiple generations from various LLMs and track outputs
- Compare responses based on predefined criteria or rubrics
- Visualize evaluation results and export findings
- Designed for both quantitative and qualitative analysis

## Repository Structure

```
.
├── LLM_Eval.ipynb        # Main notebook for evaluating and comparing LLM outputs
```

## Getting Started

1. **Install required packages**
   ```
   pip install openai pandas matplotlib
   ```

2. **Open the notebook**
   Launch Jupyter Notebook or JupyterLab and open `LLM_Eval.ipynb`.

3. **Customize**
   Adjust the model API, input prompts, and evaluation logic based on your use case.

